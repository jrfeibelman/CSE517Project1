{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ridge import ridge\n",
    "from hinge import hinge\n",
    "from logistic import logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkgradHingeAndRidge(f, X, e, x, y,lambdaa):\n",
    "# % checkgradHingeAndRidge checks the derivatives in a hinge or ridge function, by comparing them to finite\n",
    "# % differences approximations. The partial derivatives and the approximation\n",
    "# % are printed and the norm of the difference divided by the norm of the sum is\n",
    "# % returned as an indication of accuracy.\n",
    "# %\n",
    "# % usage: checkgradHingeAndRidge('f', X, e, x, y,lambdaa)\n",
    "# %\n",
    "# % where X is the argument and e is the small perturbation used for the finite\n",
    "# % differences. and the x, y,lambdaa are parameters which\n",
    "# % get passed to hinge or ridge function. The function hinge or ridge function should be of the type\n",
    "# %\n",
    "# % fX, dfX = hinge(X, x, y,lambdaa) or fX, dfX = ridge(X, x, y,lambdaa)\n",
    "# %\n",
    "# % where fX is the function value and dfX is a vector of partial derivatives.\n",
    "# %\n",
    "# % Carl Edward Rasmussen, 2001-08-01.\n",
    "\n",
    "\n",
    "    y0,dy = f(X,x,y,lambdaa)   # get the partial derivatives dy\n",
    "    dh = np.zeros((len(X),1))\n",
    "    for j in range(len(X)):\n",
    "        dx = np.zeros((len(X),1))\n",
    "        dx[j] = dx[j] + e                            # perturb a single dimension\n",
    "        y2,dy2 = f(X+dx,x,y,lambdaa)\n",
    "        dx = -dx\n",
    "        y1,dy1 = f(X+dx,x,y,lambdaa)\n",
    "        dh[j] = (y2 - y1)/(2*e)\n",
    "\n",
    "    # dh (the gradient calculated by the finite difference method) should be almost the same as dy (the gradient calculated by your function)\n",
    "    # print(\"dh:\", dh)\n",
    "    # print(\"dy:\", dy)\n",
    "\n",
    "    d = np.linalg.norm(dh-dy)/np.linalg.norm(dh+dy);       # return norm of diff divided by norm of sum\n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ridge(w,xTr,yTr,lambdaa):\n",
    "# #\n",
    "# # INPUT:\n",
    "# # w weight vector (default w=0)\n",
    "# # xTr:dxn matrix (each column is an input vector)\n",
    "# # yTr:1xn matrix (each entry is a label)\n",
    "# # lambdaa: regression constant\n",
    "# #\n",
    "# # OUTPUTS:\n",
    "# # loss = the total loss obtained with w on xTr and yTr\n",
    "# # gradient = the gradient at w\n",
    "# #\n",
    "# # [d,n]=size(xTr);\n",
    "\n",
    "#     # YOUR CODE HERE\n",
    "#     loss = (((xTr.transpose() @ w) - yTr)**2).sum() + lambdaa * (w.transpose() @ w)\n",
    "#     gradient = 2 * (((xTr.transpose() @ w) - yTr) @ xTr.transpose()).sum() + 2 * lambdaa * w\n",
    "\n",
    "#     return loss,gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N=50\n",
    "D=5\n",
    "x=np.concatenate((np.random.randn(D,N),np.random.randn(D,N)+2),axis=1)\n",
    "y=np.concatenate((np.ones((1,N)),-np.ones((1,N))),axis=1)\n",
    "d=checkgradHingeAndRidge(hinge,np.random.rand(D, 1), 1e-05, x,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Test 1\n",
      "\n",
      "Completed Test 1\n",
      "\n",
      "Starting Test 3\n",
      "\n",
      "Completed Test 3\n",
      "\n",
      "Starting Test 4\n",
      "\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "(1, 1) (5, 1)\n",
      "Completed Test 4\n",
      "\n",
      "Number of failed example tests: 0\n",
      "Number of passed example tests: 3\n",
      "\n",
      "Note: we only implemented 3 out of 12 tests for you. Check the inline documentation for what the other tests do and implement them yourself!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from logistic import logistic\n",
    "from ridge import ridge\n",
    "from checkgradLogistic import checkgradLogistic\n",
    "from checkgradHingeAndRidge import checkgradHingeAndRidge\n",
    "\n",
    "def example_tests():\n",
    "# % def example_tests():\n",
    "# %\n",
    "# % Tests for the SRM project. Some few example tests are implemented.\n",
    "# % Some are only dewscribed in the comments. You will have to implement\n",
    "# % those yourself.\n",
    "# %\n",
    "# % Output:\n",
    "# % r:    number of tests that broke\n",
    "# % ok:   number of passed tests\n",
    "# % s:    statement describing the failed test (s={} if all succeed)\n",
    "\n",
    "\n",
    "    random.seed(31415926535)\n",
    "    # % initial outputs\n",
    "    r=0\n",
    "    ok=0\n",
    "    s=[]  #used to be matlab cell array\n",
    "\n",
    "    # data set\n",
    "    N=50\n",
    "    D=5\n",
    "\n",
    "    x=np.concatenate((np.random.randn(D,N),np.random.randn(D,N)+2),axis=1)\n",
    "    y=np.concatenate((np.ones((1,N)),-np.ones((1,N))),axis=1)\n",
    "\n",
    "    print ('Starting Test 1\\n')\n",
    "    #Test 1: testing gradient of logistic\n",
    "    d=checkgradLogistic(logistic,np.random.rand(D,1),1e-05,x,y)\n",
    "    failtest = d>1e-10\n",
    "\n",
    "    if failtest:\n",
    "        r=r+1\n",
    "        s.append('Test 1: Logistic function does not pass checkgrad.')\n",
    "    else:\n",
    "        ok=ok+1;\n",
    "\n",
    "    print('Completed Test 1\\n')\n",
    "\n",
    "#     %% Test 2: logistic sanity check #1\n",
    "#     % we will tes/m label in {-1,1}. The expected outcome is (very close to) log(2).\n",
    "\n",
    "    print('Starting Test 3\\n')\n",
    "    #Test 3: logistic sanity check #2\n",
    "    w=np.random.rand(5,1)\n",
    "    logistic_loss = logistic(w,x[:,1].reshape((5,1)),np.ones((1,1)))[0]\n",
    "    failtest = np.abs(w.T.dot(x[:,1])+math.log(math.exp(logistic_loss)-1)) > 2.2204e-15\n",
    "    if failtest:\n",
    "        r=r+1\n",
    "        s.append('Test 3: Logistic function does not pass sanity check #2.')\n",
    "    else:\n",
    "        ok=ok+1\n",
    "    print('Completed Test 3\\n')\n",
    "\n",
    "\n",
    "    print('Starting Test 4\\n')\n",
    "    #Test 4: testing gradient of ridge\n",
    "    d = checkgradHingeAndRidge(ridge,np.random.rand(D, 1), 1e-05, x,y,10)\n",
    "    failtest = d > 1e-10\n",
    "\n",
    "    if failtest:\n",
    "        r = r+1\n",
    "        s.append('Test 4: Ridge function does not pass checkgrad.')\n",
    "    else:\n",
    "        ok=ok+1\n",
    "    print('Completed Test 4\\n')\n",
    "\n",
    "#     %% Test 5: testing gradient of hinge\n",
    "#     % we will test hinge using checkgrad on randomly generated x and y data\n",
    "#     % initializing w with 1e-05 and lambda with 1e-05. The gradient is supposed\n",
    "#     % to be smaller than 5e-07.\n",
    "#\n",
    "#\n",
    "#     %% Test 6: checking gradient descent\n",
    "#     % we will check grdescent using the squared loss, randomly generated input\n",
    "#     % weights and stepsize=1e-05, maxiter=1000,and tolerance=1e-09. The\n",
    "#     % norm of the gradient at the optimal solution should be zero (< 1e-05).\n",
    "#\n",
    "#\n",
    "#     %% Tests 7-12: solutions of hinge, ridge, and logistic\n",
    "#     % we will compare the solutions (loss value and gradient) of hinge,\n",
    "#     % ridge, and logistic to our implementation using x and y.\n",
    "#     % Note that you cannot implement those tests.\n",
    "\n",
    "    return r,ok,s\n",
    "\n",
    "def squaredloss(w,x,y):\n",
    "    [d,n]=np.shape(x)\n",
    "    diff=(w.T.dot(x)-y)\n",
    "    gradient=2*x.dot(diff.T)/n\n",
    "    loss = np.mean(diff**2)\n",
    "    return loss,gradient\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    failed,ok,msgs = example_tests()\n",
    "    print(\"Number of failed example tests: \"+str(failed))\n",
    "    print(\"Number of passed example tests: \"+str(ok))\n",
    "    if len(msgs):\n",
    "        failMsg = 'Unfortunately, you failed %d test(s) on this evaluation: \\n\\n' % len(msgs)\n",
    "        for j in range(0,len(msgs)):\n",
    "            print(msgs[j])\n",
    "    print(\"\\nNote: we only implemented 3 out of 12 tests for you. Check the inline documentation for what the other tests do and implement them yourself!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
